{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "source-code-knowledge-distillation.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "wf6jWufHhjHj",
        "YEoWDW2u_LLq",
        "ZM55T4jH9COb",
        "lPBD8kjazJrA",
        "hN0ZhweC-tBj",
        "fez4DhCUAIt7"
      ],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "o8rtMbBf66Cs"
      },
      "source": [
        "#libera memória GPU\n",
        "torch.cuda.empty_cache()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z-VNxBDbxXPd"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N7NHnqx--Fvj"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ot1w7RlwaEIJ"
      },
      "source": [
        "from collections import OrderedDict\n",
        "import os\n",
        "from skimage.util import random_noise\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from random import shuffle\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "from numpy import linalg as LA\n",
        "import random\n",
        "import time\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset,DataLoader\n",
        "from PIL import Image\n",
        "import torchvision\n",
        "from torchvision import models\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "from matplotlib import pyplot as plt\n",
        "import math\n",
        "from torchvision import transforms\n",
        "from torch.utils.data.sampler import SubsetRandomSampler, BatchSampler, RandomSampler\n",
        "import torch.nn as nn\n",
        "from torchvision import transforms, datasets, models\n",
        "import pandas as pd\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from torchsummary import summary\n",
        "from random import randint\n",
        "from __future__ import print_function\n",
        "from __future__ import division\n",
        "import copy\n",
        "import csv\n",
        "import sklearn\n",
        "import matplotlib as mpl\n",
        "from scipy.stats import rankdata\n",
        "RS = 23\n",
        "\n",
        "torch.backends.cudnn.deterministic = True\n",
        "random.seed(1)\n",
        "torch.manual_seed(1)\n",
        "torch.cuda.manual_seed(1)\n",
        "np.random.seed(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h6zww49l-JHO"
      },
      "source": [
        "# Preparação dos dados"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jvbVtHHfI6nw"
      },
      "source": [
        "O dataset deve estar em formato zip no google drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ze6uNcoHn-XT"
      },
      "source": [
        "# monta o google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "upA9Rss5MtVM"
      },
      "source": [
        "!unzip /content/drive/My\\ Drive/chest_xray.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_16C3Qay1KTd"
      },
      "source": [
        "!rm -rf /content/chest_xray-bal"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6xPogV_G-TCl"
      },
      "source": [
        "# Funções"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QdWG6pfWa-k0"
      },
      "source": [
        "def prepararDados(input_size):\n",
        "  datadir = '/content/chest_xray'\n",
        "  traindir = datadir + '/train/'\n",
        "  validdir = datadir + '/val/'\n",
        "  testdir = datadir + '/test/'\n",
        " \n",
        "  batch_size = 32\n",
        "\n",
        "  image_transforms = {\n",
        "    'train': transforms.Compose([    \n",
        "        transforms.Resize((input_size,input_size)),        \n",
        "        transforms.RandomHorizontalFlip(), \n",
        "        transforms.RandomResizedCrop((input_size,input_size)),       \n",
        "        transforms.ToTensor(), \n",
        "      ]),\n",
        "\n",
        "    'test': transforms.Compose([\n",
        "        transforms.Resize((input_size,input_size)),      \n",
        "        transforms.ToTensor(),\n",
        "            ]),\n",
        "\n",
        "    'val': transforms.Compose([\n",
        "        transforms.Resize((input_size,input_size)),       \n",
        "        transforms.ToTensor(),\n",
        "            ])\n",
        "  }\n",
        "  \n",
        "  # Datasets\n",
        "  data = {\n",
        "    'train':\n",
        "    datasets.ImageFolder(root=traindir, transform=image_transforms['train']),\n",
        "    'val':\n",
        "    datasets.ImageFolder(root=testdir, transform=image_transforms['val']),\n",
        "    'test':\n",
        "    datasets.ImageFolder(root=testdir, transform=image_transforms['test'])\n",
        "  } \n",
        "\n",
        "\n",
        "  dataloaders = {\n",
        "      'train': DataLoader(data['train'], shuffle=True, batch_size=batch_size, num_workers=0),\n",
        "      'val': DataLoader(data['val'], batch_size=batch_size, shuffle=True),\n",
        "      'test': DataLoader(data['test'], batch_size=batch_size, shuffle=True)\n",
        "  }\n",
        "  return dataloaders\n",
        "\n",
        "def create_tensor(shape = [2,2,2]):\n",
        "    ndim = len(shape)\n",
        "    view_shape = [1,] * ndim\n",
        "    res = 0.\n",
        "    for d in range(ndim):\n",
        "        dim_size = shape[d]\n",
        "        t = torch.arange(dim_size)\n",
        "        view_shape[d] = dim_size\n",
        "        t = t.view(*view_shape).expand(*shape)\n",
        "        view_shape[d] = 1\n",
        "        res += t\n",
        "\n",
        "    return res\n",
        "\n",
        "def testeDoModelo(model, data_loader):\n",
        "  if (torch.cuda.is_available()):\n",
        "    model = model.cuda()\n",
        "  model.eval()\n",
        "  running_loss = 0.0\n",
        "  running_correct = 0\n",
        "  tot_predictions = []\n",
        "  labels = []\n",
        "  for batch in data_loader:\n",
        "    data , target = batch\n",
        "    if torch.cuda.is_available():\n",
        "      data,target = data.cuda(),target.cuda()\n",
        "    \n",
        "    output = model(data)\n",
        "    loss = F.cross_entropy(output,target)\n",
        "    running_loss += loss.item()\n",
        "    preds = output.data.max(dim=1,keepdim=True)[1]\n",
        "    running_correct += preds.eq(target.data.view_as(preds)).cpu().sum().item()\n",
        "    \n",
        "    tot_predictions.extend(preds.data.cpu().numpy())\n",
        "    labels.extend(target.data.cpu().numpy())\n",
        "  loss = running_loss/len(data_loader.dataset)\n",
        "  accuracy = 100. * running_correct/len(data_loader.dataset)\n",
        "  print(f'Loss is {loss:{5}.{2}} Accuracy is {running_correct}/{len(data_loader.dataset)}{accuracy:{6}.{4}}%\\n')\n",
        "  return tot_predictions, labels\n",
        "\n",
        "\n",
        "def testeAgrupamento(resnet, vgg, densenet, alexnet, googlenet, data_loader):\n",
        "  if (torch.cuda.is_available()):\n",
        "    resnet = resnet.cuda()\n",
        "    vgg = vgg.cuda()\n",
        "    densenet = densenet.cuda()\n",
        "    alexnet = alexnet.cuda()\n",
        "    googlenet = googlenet.cuda()\n",
        "  \n",
        "  resnet.eval()\n",
        "  densenet.eval()\n",
        "  vgg.eval()\n",
        "  alexnet.eval()\n",
        "  googlenet.eval()\n",
        "\n",
        "  running_loss = 0.0\n",
        "  running_corrects = 0\n",
        "  tot_predictions = []\n",
        "  labels = []\n",
        "  for batch in data_loader:\n",
        "    data , target = batch\n",
        "    if torch.cuda.is_available():\n",
        "      data,target = data.cuda(),target.cuda()\n",
        "    \n",
        "    output_resnet = resnet(data)\n",
        "    output_densenet = densenet(data)\n",
        "    output_alexnet = alexnet(data)\n",
        "    output_googlenet = googlenet(data)\n",
        "    output_vgg = vgg(data)\n",
        "    \n",
        "    # #resize for inception    \n",
        "    # data = data.cpu()   \n",
        "    # data_resized = create_tensor(shape=[len(data), 3, 299, 299])\n",
        "    # for i in range(len(data)):      \n",
        "    #   img_PIL = transforms.ToPILImage()(data[i])\n",
        "    #   img_PIL = torchvision.transforms.Resize([299,299])(img_PIL)\n",
        "    #   data_resized[i] = torchvision.transforms.ToTensor()(img_PIL) \n",
        "\n",
        "    # data_resized = data_resized.cuda()\n",
        "    # output_inception = inception(data_resized)\n",
        "\n",
        "    preds_resnet = output_resnet.data.max(dim=1,keepdim=True)[1]\n",
        "    preds_densenet = output_densenet.data.max(dim=1,keepdim=True)[1]\n",
        "    preds_vgg = output_vgg.data.max(dim=1,keepdim=True)[1]\n",
        "    preds_googlenet = output_googlenet.data.max(dim=1,keepdim=True)[1]\n",
        "    preds_alexnet = output_alexnet.data.max(dim=1,keepdim=True)[1]\n",
        "\n",
        "    final_preds = []\n",
        "    for i in range(len(preds_resnet)):\n",
        "      item_pred = [preds_resnet[i].item(), preds_densenet[i].item(), preds_vgg[i].item(), preds_googlenet[i].item(), preds_alexnet[i].item()]\n",
        "      soma = sum(item_pred)\n",
        "      if soma >= 3:\n",
        "        final_preds.append(1)\n",
        "      else: \n",
        "        final_preds.append(0)\n",
        "\n",
        "    for i in range(len(target)):\n",
        "      if target[i] == final_preds[i]: \n",
        "        running_corrects += 1\n",
        "    \n",
        "    tot_predictions.extend(final_preds)\n",
        "    labels.extend(target.data.cpu().numpy())\n",
        "    \n",
        "\n",
        "  accuracy = 100. * running_corrects/len(data_loader.dataset)\n",
        "  print(f'Accuracy is {running_corrects}/{len(data_loader.dataset)}{accuracy:{6}.{4}}%\\n')\n",
        "  return tot_predictions, labels\n",
        "  \n",
        "\n",
        "def testeAgrupamentoMediaLogits(resnet, vgg, densenet, alexnet, googlenet, data_loader):\n",
        "  if (torch.cuda.is_available()):\n",
        "    resnet = resnet.cuda()\n",
        "    vgg = vgg.cuda()\n",
        "    densenet = densenet.cuda()\n",
        "    alexnet = alexnet.cuda()\n",
        "    googlenet = googlenet.cuda()\n",
        "  \n",
        "  resnet.eval()\n",
        "  densenet.eval()\n",
        "  vgg.eval()\n",
        "  alexnet.eval()\n",
        "  googlenet.eval()\n",
        "\n",
        "  running_loss = 0.0\n",
        "  running_corrects = 0\n",
        "  tot_predictions = []\n",
        "  labels = []\n",
        "  for batch in data_loader:\n",
        "    data , target = batch\n",
        "    if torch.cuda.is_available():\n",
        "      data,target = data.cuda(),target.cuda()\n",
        "    \n",
        "    output_resnet = resnet(data)\n",
        "    output_densenet = densenet(data)\n",
        "    output_alexnet = alexnet(data)\n",
        "    output_googlenet = googlenet(data)\n",
        "    output_vgg = vgg(data)\n",
        "\n",
        "    outputs = (output_resnet+output_densenet+output_alexnet+output_googlenet+output_vgg)/5\n",
        "      \n",
        "    _, preds = torch.max(outputs, 1)\n",
        "    \n",
        "\n",
        "    for i in range(len(target)):\n",
        "      if target[i] == preds[i]: \n",
        "        running_corrects += 1\n",
        "\n",
        "    final_preds = []\n",
        "    for i in range(len(preds)):\n",
        "      final_preds.append(preds[i].item())\n",
        "    \n",
        "    tot_predictions.extend(final_preds)\n",
        "    labels.extend(target.data.cpu().numpy())\n",
        "  \n",
        "\n",
        "  accuracy = 100. * running_corrects/len(data_loader.dataset)\n",
        "  print(f'Accuracy is {running_corrects}/{len(data_loader.dataset)}{accuracy:{6}.{4}}%\\n')\n",
        "  return tot_predictions, labels\n",
        "\n",
        "\n",
        "def metricasModelo(model, data_loader):\n",
        "  if (torch.cuda.is_available()):\n",
        "    model = model.cuda()\n",
        "    \n",
        "  print('========= Métricas do modelo ============\\n')\n",
        "  y_pred, y_test = testeDoModelo(model, data_loader)\n",
        "  target_names = ['normal', 'pneumonia']\n",
        "  print(classification_report(y_test, y_pred, target_names=target_names, digits=3))\n",
        "  print('\\n========= Confusion Matrix ============\\n')\n",
        "  confmat = confusion_matrix(y_true=y_test, y_pred=y_pred, labels=[0,1])\n",
        "  fig, ax = plt.subplots(figsize=(3,3))\n",
        "  ax.matshow(confmat, cmap=plt.cm.Blues, alpha=0.3)\n",
        "  for i in range(confmat.shape[0]):\n",
        "      for j in range(confmat.shape[1]):\n",
        "          ax.text(x=j, y=i, s=confmat[i, j], va='center', ha='center')\n",
        "  plt.xlabel('predicted label')\n",
        "  plt.ylabel('true label')\n",
        "  plt.tight_layout()\n",
        "  plt.savefig('confusion_matrix.png', dpi=300)\n",
        "  plt.show()\n",
        "\n",
        "def carregar_resnet():\n",
        "  resnet, input_size = initialize_model('resnet', 2, True, use_pretrained=True)\n",
        "  resnet.load_state_dict(torch.load('/content/drive/My Drive/Modelos Salvos/Resnet18/resnet.acc'))\n",
        "  return resnet, input_size\n",
        "\n",
        "def carregar_densenet():\n",
        "  densenet, input_size = initialize_model('densenet', 2, True, use_pretrained=True)\n",
        "  densenet.load_state_dict(torch.load('/content/drive/My Drive/Modelos Salvos/Densenet121/densenet.acc'))\n",
        "  return densenet, input_size\n",
        "\n",
        "def carregar_alexnet():\n",
        "  alexnet, input_size = initialize_model('alexnet', 2, True, use_pretrained=True)\n",
        "  alexnet.load_state_dict(torch.load('/content/drive/My Drive/Modelos Salvos/AlexNet/alexnet.acc'))\n",
        "  return alexnet, input_size\n",
        "\n",
        "def carregar_googlenet():\n",
        "  googlenet, input_size = initialize_model('googlenet', 2, True, use_pretrained=True)\n",
        "  googlenet.load_state_dict(torch.load('/content/drive/My Drive/Modelos Salvos/GoogLeNet/googlenet.acc'))\n",
        "  return googlenet, input_size\n",
        "\n",
        "def carregar_inception():\n",
        "  inception, input_size = initialize_model('inception', 2, True, use_pretrained=True)\n",
        "  inception.load_state_dict(torch.load('/content/drive/My Drive/Modelos Salvos/Inception V3/inception.acc'))\n",
        "  return inception, input_size\n",
        "\n",
        "def carregar_vgg():\n",
        "  vgg = models.vgg19_bn(pretrained=True)\n",
        "  num_ftrs = vgg.classifier[6].in_features\n",
        "  vgg.classifier[6] = nn.Linear(num_ftrs, 2)\n",
        "  vgg.load_state_dict(torch.load('/content/drive/My Drive/Modelos Salvos/VGG/vgg.acc'))\n",
        "  return vgg, 224\n",
        "\n",
        "def carregar_cnn(model_save_name):\n",
        "    model = Network()    \n",
        "    model.load_state_dict(torch.load(model_save_name))\n",
        "    return model\n",
        "\n",
        "def carregar_ensemble():\n",
        "  #load the models\n",
        "  resnet, input_size = carregar_resnet()\n",
        "  vgg, input_size = carregar_vgg()\n",
        "  densenet, input_size = carregar_densenet()\n",
        "  alexnet, input_size = carregar_alexnet()\n",
        "  googlenet, input_size = carregar_googlenet()\n",
        "  ensemble = {'resnet': resnet, 'vgg': vgg, 'densenet': densenet, 'alexnet': alexnet, 'googlenet': googlenet}\n",
        "  return ensemble\n",
        "\n",
        "def salvar_modelo(model, model_save_name):\n",
        "    torch.save(model.state_dict(), model_save_name)  \n",
        "\n",
        "def plotar(train_loss, train_acc, val_loss, val_acc):\n",
        "  plt.title('Accuracy over epochs')\n",
        "  plt.xlabel('Epochs')\n",
        "  plt.ylabel('Accuracy')\n",
        "  plt.plot(train_acc, label='train')\n",
        "  plt.plot(val_acc, label='val')\n",
        "  plt.grid(True, which='both', axis='both')  \n",
        "  plt.tight_layout()\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "  plt.title('Loss over epochs')\n",
        "  plt.xlabel('Epochs')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.plot(train_loss, label='train')\n",
        "  plt.plot(val_loss, label='val')\n",
        "  plt.grid(True, which='both', axis='both')\n",
        "  plt.tight_layout()\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "def train_model(model, dataloaders, criterion, optimizer, save_name, num_epochs=25,  is_inception=False):\n",
        "  since = time.time()\n",
        "  \n",
        "  # informações para plotagem\n",
        "  train_losses , train_accuracy = [],[]\n",
        "  val_losses , val_accuracy = [],[]\n",
        "\n",
        "  # Gerencia a Learning Rate\n",
        "  scheduler = StepLR(optimizer, step_size=3, gamma=0.5)\n",
        "\n",
        "  val_acc_history = []\n",
        "\n",
        "  best_model_wts = copy.deepcopy(model.state_dict())\n",
        "  best_acc = 0.0\n",
        " \n",
        "\n",
        "  for epoch in range(1, num_epochs+1):\n",
        "    print('Epoch {}/{}'.format(epoch, num_epochs))\n",
        "    print('-' * 10)\n",
        "\n",
        "    epochStart = time.time()  \n",
        "\n",
        "    # Each epoch has a training and validation phase\n",
        "    for phase in ['train', 'val']:\n",
        "            \n",
        "      if phase == 'train':\n",
        "          model.train()  # Set model to training mode\n",
        "      else:\n",
        "          model.eval()   # Set model to evaluate mode\n",
        "\n",
        "      running_loss = 0.0\n",
        "      running_corrects = 0\n",
        "      epoch_loss = 0.0\n",
        "      epoch_acc = 0.0\n",
        "\n",
        "      # Iterate over data.\n",
        "      for inputs, labels in dataloaders[phase]:\n",
        "\n",
        "        if phase == 'train':\n",
        "          for i in range(len(inputs)): \n",
        "            noise = randint(0,2)\n",
        "            # Se for 0 não aplica ruído à imagem\n",
        "            if noise == 1:\n",
        "              # aplica ruído gaussiano à imagem\n",
        "              inputs[i] = torch.tensor(random_noise(inputs[i], mode='gaussian', mean=0, var=0.05, clip=True))\n",
        "            if noise == 2:\n",
        "              # aplica speckle noise à imagem\n",
        "              inputs[i] = torch.tensor(random_noise(inputs[i], mode='speckle', mean=0, var=0.05, clip=True))\n",
        "\n",
        "\n",
        "        inputs = inputs.cuda()\n",
        "        labels = labels.cuda()\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward\n",
        "        # track history if only in train\n",
        "        with torch.set_grad_enabled(phase == 'train'):\n",
        "          # Get model outputs and calculate loss\n",
        "          # Special case for inception because in training it has an auxiliary output. In train\n",
        "          #   mode we calculate the loss by summing the final output and the auxiliary output\n",
        "          #   but in testing we only consider the final output.\n",
        "          if is_inception and phase == 'train':\n",
        "            # From https://discuss.pytorch.org/t/how-to-optimize-inception-model-with-auxiliary-classifiers/7958\n",
        "            outputs, aux_outputs = model(inputs)\n",
        "            loss1 = criterion(outputs, labels)\n",
        "            loss2 = criterion(aux_outputs, labels)\n",
        "            loss = loss1 + 0.4*loss2\n",
        "          else:\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "          _, preds = torch.max(outputs, 1)\n",
        "\n",
        "          # backward + optimize only if in training phase\n",
        "          if phase == 'train':\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "          # statistics\n",
        "          running_loss += loss.item() * inputs.size(0)\n",
        "          running_corrects += torch.sum(preds == labels.data)\n",
        "        \n",
        "      epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
        "      epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)      \n",
        "      \n",
        "      if phase == 'train':\n",
        "        train_losses.append(epoch_loss)\n",
        "        train_accuracy.append(epoch_acc.item())         \n",
        "      else:\n",
        "        val_losses.append(epoch_loss)\n",
        "        val_accuracy.append(epoch_acc.item())    \n",
        "\n",
        "      print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
        "      \n",
        "\n",
        "      # deep copy the model\n",
        "      if phase == 'val' and epoch_acc > best_acc:\n",
        "        best_acc = epoch_acc\n",
        "        best_model_wts = copy.deepcopy(model.state_dict())\n",
        "        salvar_modelo(model, '/content/drive/My Drive/Modelos Salvos/'+save_name+'.acc')\n",
        "        print('Val acc up saving model')\n",
        "      if phase == 'val':\n",
        "        val_acc_history.append(epoch_acc)    \n",
        "    scheduler.step()\n",
        "    epochEnd = time.time()\n",
        "    print('Duração:', (epochEnd - epochStart))\n",
        "  time_elapsed = time.time() - since\n",
        "  print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
        "  print('Best val Acc: {:4f}'.format(best_acc))\n",
        "  \n",
        "  #plota os gráficos\n",
        "  plotar(train_losses, train_accuracy, val_losses, val_accuracy)\n",
        "\n",
        "  #salvar dados da plotagem\n",
        "  train = {'loss': train_losses, 'acc': train_accuracy}\n",
        "  valid = {'loss': val_losses, 'acc': val_accuracy}\n",
        "\n",
        "  df = pd.DataFrame(data=train)\n",
        "  df.to_csv('/content/drive/My Drive/Modelos Salvos/'+save_name+'_train.csv')\n",
        "\n",
        "  df = pd.DataFrame(data=valid)\n",
        "  df.to_csv('/content/drive/My Drive/Modelos Salvos/'+save_name+'_valid.csv')\n",
        "  \n",
        "  # load best model weights\n",
        "  model.load_state_dict(best_model_wts)\n",
        "  return model, val_acc_history\n",
        "\n",
        "\n",
        "def set_parameter_requires_grad(model, feature_extracting):\n",
        "  if feature_extracting:\n",
        "    for param in model.parameters():\n",
        "      param.requires_grad = False\n",
        "\n",
        "def initialize_model(model_name, num_classes, feature_extract, use_pretrained=True):\n",
        "  # Initialize these variables which will be set in this if statement. Each of these\n",
        "  \n",
        "  #   variables is model specific.\n",
        "  model_ft = None\n",
        "  input_size = 0\n",
        "\n",
        "  if model_name == \"resnet\":\n",
        "    \"\"\" Resnet18\n",
        "    \"\"\"\n",
        "    model_ft = models.resnet18(pretrained=use_pretrained)\n",
        "    set_parameter_requires_grad(model_ft, feature_extract)\n",
        "    set_parameter_requires_grad(model_ft.layer4, True)\n",
        "    \n",
        "    num_ftrs = model_ft.fc.in_features\n",
        "    model_ft.fc = nn.Linear(num_ftrs, num_classes)\n",
        "    input_size = 224\n",
        "\n",
        "  elif model_name == \"alexnet\":\n",
        "    \"\"\" Alexnet\n",
        "    \"\"\"\n",
        "    model_ft = models.alexnet(pretrained=use_pretrained)\n",
        "    set_parameter_requires_grad(model_ft.features, feature_extract)\n",
        "    num_ftrs = model_ft.classifier[6].in_features\n",
        "    model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
        "    input_size = 224\n",
        "\n",
        "  elif model_name == \"vgg\":\n",
        "    \"\"\" VGG11_bn\n",
        "    \"\"\"\n",
        "    model_ft = models.vgg19_bn(pretrained=use_pretrained)\n",
        "    set_parameter_requires_grad(model_ft, feature_extract)\n",
        "    num_ftrs = model_ft.classifier[6].in_features\n",
        "    model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
        "    input_size = 224\n",
        "\n",
        "  elif model_name == \"squeezenet\":\n",
        "    \"\"\" Squeezenet\n",
        "    \"\"\"\n",
        "    model_ft = models.squeezenet1_0(pretrained=use_pretrained)\n",
        "    set_parameter_requires_grad(model_ft.features, feature_extract)\n",
        "    model_ft.classifier[1] = nn.Conv2d(512, num_classes, kernel_size=(1,1), stride=(1,1))\n",
        "    model_ft.num_classes = num_classes\n",
        "    input_size = 224\n",
        "\n",
        "  elif model_name == \"densenet\":\n",
        "    \"\"\" Densenet\n",
        "    \"\"\"\n",
        "    model_ft = models.densenet121(pretrained=use_pretrained)\n",
        "    set_parameter_requires_grad(model_ft, feature_extract)\n",
        "    num_ftrs = model_ft.classifier.in_features\n",
        "    model_ft.classifier = nn.Linear(num_ftrs, num_classes)\n",
        "    input_size = 224\n",
        "\n",
        "  elif model_name == \"inception\":\n",
        "    \"\"\" Inception v3\n",
        "    Be careful, expects (299,299) sized images and has auxiliary output\n",
        "    \"\"\"\n",
        "    model_ft = models.inception_v3(pretrained=use_pretrained)\n",
        "    set_parameter_requires_grad(model_ft, feature_extract)\n",
        "    # Handle the auxilary net\n",
        "    num_ftrs = model_ft.AuxLogits.fc.in_features\n",
        "    model_ft.AuxLogits.fc = nn.Linear(num_ftrs, num_classes)\n",
        "    # Handle the primary net\n",
        "    num_ftrs = model_ft.fc.in_features\n",
        "    model_ft.fc = nn.Linear(num_ftrs,num_classes)\n",
        "    input_size = 299\n",
        "\n",
        "  elif model_name == \"googlenet\":\n",
        "    \"\"\" GoogLeNet\n",
        "    \"\"\"\n",
        "    model_ft = models.googlenet(pretrained=use_pretrained)\n",
        "    set_parameter_requires_grad(model_ft, feature_extract)\n",
        "    num_ftrs = model_ft.fc.in_features\n",
        "    model_ft.fc = nn.Linear(num_ftrs, num_classes)\n",
        "    input_size = 224\n",
        "\n",
        "  else:\n",
        "      print(\"Invalid model name, exiting...\")\n",
        "      exit()\n",
        "\n",
        "  return model_ft, input_size\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wbFgBfCJN3Yp"
      },
      "source": [
        "# Resnet\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "le7iPTo1N7DF"
      },
      "source": [
        "model_name = \"resnet\"\n",
        "\n",
        "# Number of classes in the dataset\n",
        "num_classes = 2\n",
        "\n",
        "# Number of epochs to train for\n",
        "num_epochs = 50\n",
        "\n",
        "# Flag for feature extracting. When False, we finetune the whole model,\n",
        "#   when True we only update the reshaped layer params\n",
        "feature_extract = True\n",
        "\n",
        "model_ft, input_size = initialize_model(model_name, num_classes, feature_extract, use_pretrained=True)\n",
        "\n",
        "for param in model_ft.layer4.parameters():\n",
        "  param.requires_grad = True\n",
        "\n",
        "loaders = prepararDados(input_size)\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model_ft = model_ft.to(device)\n",
        "\n",
        "# lista dos parãmetros que serão aprendidos\n",
        "params_to_update = model_ft.parameters()\n",
        "print(\"Params to learn:\")\n",
        "if feature_extract:\n",
        "    params_to_update = []\n",
        "    for name,param in model_ft.named_parameters():\n",
        "        if param.requires_grad == True:\n",
        "            params_to_update.append(param)\n",
        "            print(\"\\t\",name)\n",
        "else:\n",
        "    for name,param in model_ft.named_parameters():\n",
        "        if param.requires_grad == True:\n",
        "            print(\"\\t\",name)\n",
        "\n",
        "# Observe that all parameters are being optimized\n",
        "optimizer_ft = optim.Adam(params_to_update, lr=0.001, weight_decay=0.000125)\n",
        "\n",
        "# Setup the loss fxn\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K3MddECbOXdU"
      },
      "source": [
        "# Train and evaluate\n",
        "model_ft, hist = train_model(model_ft, loaders, criterion, optimizer_ft, model_name, num_epochs=num_epochs, is_inception=(model_name==\"inception\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uA6mHpMWOXlY"
      },
      "source": [
        "resnet, input_size = carregar_resnet()\n",
        "loaders = prepararDados(input_size)\n",
        "print('Train set')\n",
        "metricasModelo(resnet, loaders['train'])\n",
        "print('Test set')\n",
        "metricasModelo(resnet, loaders['test'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wf6jWufHhjHj"
      },
      "source": [
        "# Inception V3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "07IkM_AoWV6h"
      },
      "source": [
        "model_name = \"inception\"\n",
        "\n",
        "# Number of classes in the dataset\n",
        "num_classes = 2\n",
        "\n",
        "# Number of epochs to train for\n",
        "num_epochs = 50\n",
        "\n",
        "# Flag for feature extracting. When False, we finetune the whole model,\n",
        "#   when True we only update the reshaped layer params\n",
        "feature_extract = True\n",
        "\n",
        "model_ft, input_size = initialize_model(model_name, num_classes, feature_extract, use_pretrained=True)\n",
        "for param in model_ft.Mixed_7c.parameters():\n",
        "  param.requires_grad = True\n",
        "\n",
        "loaders = prepararDados(input_size)\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model_ft = model_ft.to(device)\n",
        "\n",
        "# lista dos parãmetros que serão aprendidos\n",
        "params_to_update = model_ft.parameters()\n",
        "print(\"Params to learn:\")\n",
        "if feature_extract:\n",
        "    params_to_update = []\n",
        "    for name,param in model_ft.named_parameters():\n",
        "        if param.requires_grad == True:\n",
        "            params_to_update.append(param)\n",
        "            print(\"\\t\",name)\n",
        "else:\n",
        "    for name,param in model_ft.named_parameters():\n",
        "        if param.requires_grad == True:\n",
        "            print(\"\\t\",name)\n",
        "\n",
        "# Observe that all parameters are being optimized\n",
        "optimizer_ft = optim.Adam(params_to_update, lr=0.001, weight_decay=0.000125)\n",
        "\n",
        "# Setup the loss fxn\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ObNYfSesXLcq"
      },
      "source": [
        "# Train and evaluate\n",
        "model_ft, hist = train_model(model_ft, loaders, criterion, optimizer_ft, model_name, num_epochs=num_epochs, is_inception=(model_name==\"inception\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tDZ3y_xu783w"
      },
      "source": [
        "inception, input_size = carregar_inception()\n",
        "loaders = prepararDados(input_size)\n",
        "print('Train set')\n",
        "metricasModelo(inception, loaders['train'])\n",
        "print('Test set')\n",
        "metricasModelo(inception, loaders['test'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XiCCIVxvI-L1"
      },
      "source": [
        "# DenseNet\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wBfLWl6sJErl"
      },
      "source": [
        "model_name = \"densenet\"\n",
        "\n",
        "# Number of classes in the dataset\n",
        "num_classes = 2\n",
        "\n",
        "# Number of epochs to train for\n",
        "num_epochs = 50\n",
        "\n",
        "# Flag for feature extracting. When False, we finetune the whole model,\n",
        "#   when True we only update the reshaped layer params\n",
        "feature_extract = True\n",
        "\n",
        "model_ft, input_size = initialize_model(model_name, num_classes, feature_extract, use_pretrained=True)\n",
        "\n",
        "for param in model_ft.features.denseblock4.denselayer15.parameters():\n",
        "  param.requires_grad = True\n",
        "\n",
        "for param in model_ft.features.denseblock4.denselayer16.parameters():\n",
        "  param.requires_grad = True\n",
        "\n",
        "loaders = prepararDados(input_size)\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model_ft = model_ft.to(device)\n",
        "\n",
        "# lista dos parãmetros que serão aprendidos\n",
        "params_to_update = model_ft.parameters()\n",
        "print(\"Params to learn:\")\n",
        "if feature_extract:\n",
        "    params_to_update = []\n",
        "    for name,param in model_ft.named_parameters():\n",
        "        if param.requires_grad == True:\n",
        "            params_to_update.append(param)\n",
        "            print(\"\\t\",name)\n",
        "else:\n",
        "    for name,param in model_ft.named_parameters():\n",
        "        if param.requires_grad == True:\n",
        "            print(\"\\t\",name)\n",
        "\n",
        "# Observe that all parameters are being optimized\n",
        "optimizer_ft = optim.Adam(params_to_update, lr=0.001, weight_decay=0.000125)\n",
        "\n",
        "# Setup the loss fxn\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UWNZOufspW79"
      },
      "source": [
        "# Train and evaluate\n",
        "model_ft, hist = train_model(model_ft, loaders, criterion, optimizer_ft, model_name, num_epochs=num_epochs, is_inception=(model_name==\"inception\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ige6aC1-6zN1"
      },
      "source": [
        "densenet, input_size = carregar_densenet()\n",
        "loaders = prepararDados(input_size)\n",
        "print('Train set')\n",
        "metricasModelo(densenet, loaders['train'])\n",
        "print('Test set')\n",
        "metricasModelo(densenet, loaders['test'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YEoWDW2u_LLq"
      },
      "source": [
        "# AlexNet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6kigfpf1_NYz"
      },
      "source": [
        "model_name = \"alexnet\"\n",
        "\n",
        "# Number of classes in the dataset\n",
        "num_classes = 2\n",
        "\n",
        "# Number of epochs to train for\n",
        "num_epochs = 50\n",
        "\n",
        "# Flag for feature extracting. When False, we finetune the whole model,\n",
        "#   when True we only update the reshaped layer params\n",
        "feature_extract = True\n",
        "\n",
        "model_ft, input_size = initialize_model(model_name, num_classes, feature_extract, use_pretrained=True)\n",
        "\n",
        "loaders = prepararDados(input_size)\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model_ft = model_ft.to(device)\n",
        "\n",
        "# lista dos parãmetros que serão aprendidos\n",
        "params_to_update = model_ft.parameters()\n",
        "print(\"Params to learn:\")\n",
        "if feature_extract:\n",
        "    params_to_update = []\n",
        "    for name,param in model_ft.named_parameters():\n",
        "        if param.requires_grad == True:\n",
        "            params_to_update.append(param)\n",
        "            print(\"\\t\",name)\n",
        "else:\n",
        "    for name,param in model_ft.named_parameters():\n",
        "        if param.requires_grad == True:\n",
        "            print(\"\\t\",name)\n",
        "\n",
        "# Observe that all parameters are being optimized\n",
        "optimizer_ft = optim.Adam(params_to_update, lr=0.001, weight_decay=0.000125)\n",
        "\n",
        "# Setup the loss fxn\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kq76SUCc_Ndm"
      },
      "source": [
        "# Train and evaluate\n",
        "model_ft, hist = train_model(model_ft, loaders, criterion, optimizer_ft, model_name, num_epochs=num_epochs, is_inception=(model_name==\"inception\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7QPd9LiP7tVM"
      },
      "source": [
        "alexnet, input_size = carregar_alexnet()\n",
        "loaders = prepararDados(input_size)\n",
        "print('Train set')\n",
        "metricasModelo(alexnet, loaders['train'])\n",
        "print('Test set')\n",
        "metricasModelo(alexnet, loaders['test'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZM55T4jH9COb"
      },
      "source": [
        "# GoogLeNet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QUnGYk6A9Fg2"
      },
      "source": [
        "model_name = \"googlenet\"\n",
        "\n",
        "# Number of classes in the dataset\n",
        "num_classes = 2\n",
        "\n",
        "# Number of epochs to train for\n",
        "num_epochs = 50\n",
        "\n",
        "# Flag for feature extracting. When False, we finetune the whole model,\n",
        "#   when True we only update the reshaped layer params\n",
        "feature_extract = True\n",
        "\n",
        "model_ft = models.googlenet(pretrained=feature_extract)\n",
        "set_parameter_requires_grad(model_ft, feature_extract)\n",
        "for param in model_ft.inception5b.parameters():\n",
        "  param.requires_grad = True\n",
        "num_ftrs = model_ft.fc.in_features\n",
        "model_ft.fc = nn.Linear(num_ftrs, num_classes)\n",
        "input_size = 224\n",
        "\n",
        "loaders = prepararDados(input_size)\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model_ft = model_ft.to(device)\n",
        "\n",
        "# lista dos parãmetros que serão aprendidos\n",
        "params_to_update = model_ft.parameters()\n",
        "print(\"Params to learn:\")\n",
        "if feature_extract:\n",
        "    params_to_update = []\n",
        "    for name,param in model_ft.named_parameters():\n",
        "        if param.requires_grad == True:\n",
        "            params_to_update.append(param)\n",
        "            print(\"\\t\",name)\n",
        "else:\n",
        "    for name,param in model_ft.named_parameters():\n",
        "        if param.requires_grad == True:\n",
        "            print(\"\\t\",name)\n",
        "\n",
        "# Observe that all parameters are being optimized\n",
        "optimizer_ft = optim.Adam(params_to_update, lr=0.001, weight_decay=0.000125)\n",
        "\n",
        "# Setup the loss fxn\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rG0_diPi9Fl6"
      },
      "source": [
        "# Train and evaluate\n",
        "model_ft, hist = train_model(model_ft, loaders, criterion, optimizer_ft, model_name, num_epochs=num_epochs, is_inception=(model_name==\"inception\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZVkA1C6h9Fj8"
      },
      "source": [
        "googlenet, input_size = carregar_googlenet()\n",
        "loaders = prepararDados(input_size)\n",
        "print('Train set')\n",
        "metricasModelo(model_ft, loaders['train'])\n",
        "print('Test set')\n",
        "metricasModelo(model_ft, loaders['test'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lPBD8kjazJrA"
      },
      "source": [
        "# VGG"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kj28ootZzMvD"
      },
      "source": [
        "model_name = 'vgg'\n",
        "\n",
        "num_epochs = 50\n",
        "\n",
        "model_ft = models.vgg19_bn(pretrained=True)\n",
        "for param in model_ft.features.parameters():\n",
        "  param.requires_grad = False\n",
        "num_ftrs = model_ft.classifier[6].in_features\n",
        "model_ft.classifier[6] = nn.Linear(num_ftrs, 2)\n",
        "input_size = 224\n",
        "\n",
        "loaders = prepararDados(input_size)\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model_ft = model_ft.to(device)\n",
        "\n",
        "# lista dos parãmetros que serão aprendidos\n",
        "params_to_update = model_ft.parameters()\n",
        "print(\"Params to learn:\")\n",
        "if True:\n",
        "    params_to_update = []\n",
        "    for name,param in model_ft.named_parameters():\n",
        "        if param.requires_grad == True:\n",
        "            params_to_update.append(param)\n",
        "            print(\"\\t\",name)\n",
        "else:\n",
        "    for name,param in model_ft.named_parameters():\n",
        "        if param.requires_grad == True:\n",
        "            print(\"\\t\",name)\n",
        "\n",
        "# Observe that all parameters are being optimized\n",
        "optimizer_ft = optim.Adam(params_to_update, lr=0.001, weight_decay=0.000125)\n",
        "\n",
        "# Setup the loss fxn\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KPzaZRkWzMyg"
      },
      "source": [
        "# Train and evaluate\n",
        "model_ft, hist = train_model(model_ft, loaders, criterion, optimizer_ft, model_name, num_epochs=num_epochs, is_inception=(model_name==\"inception\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vqp9SXNVRN23"
      },
      "source": [
        "vgg, input_size = carregar_vgg()\n",
        "loaders = prepararDados(input_size)\n",
        "print('Test set')\n",
        "metricasModelo(vgg, loaders['test'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nPrhHLY3u2IC"
      },
      "source": [
        "# Agrupamento de redes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46-8NrX0u87Y"
      },
      "source": [
        "#load the models\n",
        "resnet, input_size = carregar_resnet()\n",
        "vgg, input_size = carregar_vgg()\n",
        "densenet, input_size = carregar_densenet()\n",
        "alexnet, input_size = carregar_alexnet()\n",
        "googlenet, input_size = carregar_googlenet()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QtthMoBWODVA"
      },
      "source": [
        "#evaluate the models\n",
        "loaders = prepararDados(input_size)\n",
        "\n",
        "ini = time.time()\n",
        "print('VGG19 - Test set')\n",
        "metricasModelo(vgg, loaders['test'])\n",
        "print(time.time() - ini);\n",
        "\n",
        "ini = time.time()\n",
        "print('Resnet18 - Test set')\n",
        "metricasModelo(resnet, loaders['test'])\n",
        "print(time.time() - ini);\n",
        "\n",
        "ini = time.time()\n",
        "print('Densenet121 - Test set')\n",
        "metricasModelo(densenet, loaders['test'])\n",
        "print(time.time() - ini);\n",
        "\n",
        "ini = time.time()\n",
        "print('Alexnet - Test set')\n",
        "metricasModelo(alexnet, loaders['test'])\n",
        "print(time.time() - ini);\n",
        "\n",
        "ini = time.time()\n",
        "print('GoogleNet - Test set')\n",
        "metricasModelo(googlenet, loaders['test'])\n",
        "print(time.time() - ini);\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43smFPLbOtPW"
      },
      "source": [
        "# Usando predição por votação \n",
        "loaders = prepararDados(input_size)\n",
        "print('========= Métricas do modelo ============\\n')\n",
        "ini = time.time()\n",
        "y_pred, y_test = testeAgrupamentoMediaLogits(resnet, vgg, densenet, alexnet, googlenet, loaders['test'])\n",
        "print(time.time() - ini)\n",
        "target_names = ['normal', 'pneumonia']\n",
        "print(classification_report(y_test, y_pred, target_names=target_names, digits=3))\n",
        "print('\\n========= Confusion Matrix ============\\n')\n",
        "confmat = confusion_matrix(y_true=y_test, y_pred=y_pred, labels=[0,1])\n",
        "fig, ax = plt.subplots(figsize=(3,3))\n",
        "ax.matshow(confmat, cmap=plt.cm.Blues, alpha=0.3)\n",
        "for i in range(confmat.shape[0]):\n",
        "    for j in range(confmat.shape[1]):\n",
        "        ax.text(x=j, y=i, s=confmat[i, j], va='center', ha='center')\n",
        "plt.xlabel('predicted label')\n",
        "plt.ylabel('true label')\n",
        "plt.tight_layout()\n",
        "plt.savefig('confusion_matrix.png', dpi=300)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "INM3JvQJOtPd"
      },
      "source": [
        "# teste agrupamento usando votação\n",
        "\n",
        "print('========= Métricas do modelo ============\\n')\n",
        "ini = time.time()\n",
        "y_pred, y_test = testeAgrupamento(resnet, vgg, densenet, alexnet, googlenet, loaders['test'])\n",
        "print(time.time() - ini)\n",
        "target_names = ['normal', 'pneumonia']\n",
        "print(classification_report(y_test, y_pred, target_names=target_names, digits=3))\n",
        "print('\\n========= Confusion Matrix ============\\n')\n",
        "confmat = confusion_matrix(y_true=y_test, y_pred=y_pred, labels=[0,1])\n",
        "fig, ax = plt.subplots(figsize=(3,3))\n",
        "ax.matshow(confmat, cmap=plt.cm.Blues, alpha=0.3)\n",
        "for i in range(confmat.shape[0]):\n",
        "    for j in range(confmat.shape[1]):\n",
        "        ax.text(x=j, y=i, s=confmat[i, j], va='center', ha='center')\n",
        "plt.xlabel('predicted label')\n",
        "plt.ylabel('true label')\n",
        "plt.tight_layout()\n",
        "plt.savefig('confusion_matrix.png', dpi=300)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-A4BAtJqOnf6"
      },
      "source": [
        "Teste dos modelos podados"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B5IrC3SI-Umk"
      },
      "source": [
        "#evaluate the models\n",
        "loaders = prepararDados(input_size)\n",
        "\n",
        "ini = time.time()\n",
        "print('VGG19 - Test set')\n",
        "metricasModelo(vgg, loaders['test'])\n",
        "print(time.time() - ini);\n",
        "\n",
        "ini = time.time()\n",
        "print('Resnet18 - Test set')\n",
        "metricasModelo(resnet, loaders['test'])\n",
        "print(time.time() - ini);\n",
        "\n",
        "ini = time.time()\n",
        "print('Densenet121 - Test set')\n",
        "metricasModelo(densenet, loaders['test'])\n",
        "print(time.time() - ini);\n",
        "\n",
        "ini = time.time()\n",
        "print('Alexnet - Test set')\n",
        "metricasModelo(alexnet, loaders['test'])\n",
        "print(time.time() - ini);\n",
        "\n",
        "ini = time.time()\n",
        "print('GoogleNet - Test set')\n",
        "metricasModelo(googlenet, loaders['test'])\n",
        "print(time.time() - ini);\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OjzOJ6MQwaeE"
      },
      "source": [
        "# Usando predição por votação \n",
        "loaders = prepararDados(input_size)\n",
        "print('========= Métricas do modelo ============\\n')\n",
        "ini = time.time()\n",
        "y_pred, y_test = testeAgrupamento(resnet, vgg, densenet, alexnet, googlenet, loaders['test'])\n",
        "print(time.time() - ini)\n",
        "target_names = ['normal', 'pneumonia']\n",
        "print(classification_report(y_test, y_pred, target_names=target_names, digits=3))\n",
        "print('\\n========= Confusion Matrix ============\\n')\n",
        "confmat = confusion_matrix(y_true=y_test, y_pred=y_pred, labels=[0,1])\n",
        "fig, ax = plt.subplots(figsize=(3,3))\n",
        "ax.matshow(confmat, cmap=plt.cm.Blues, alpha=0.3)\n",
        "for i in range(confmat.shape[0]):\n",
        "    for j in range(confmat.shape[1]):\n",
        "        ax.text(x=j, y=i, s=confmat[i, j], va='center', ha='center')\n",
        "plt.xlabel('predicted label')\n",
        "plt.ylabel('true label')\n",
        "plt.tight_layout()\n",
        "plt.savefig('confusion_matrix.png', dpi=300)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b5i3vAGgAr42"
      },
      "source": [
        "# teste agrupamento usando votação\n",
        "\n",
        "print('========= Métricas do modelo ============\\n')\n",
        "ini = time.time()\n",
        "y_pred, y_test = testeAgrupamento(resnet, vgg, densenet, alexnet, googlenet, loaders['test'])\n",
        "print(time.time() - ini)\n",
        "target_names = ['normal', 'pneumonia']\n",
        "print(classification_report(y_test, y_pred, target_names=target_names, digits=3))\n",
        "print('\\n========= Confusion Matrix ============\\n')\n",
        "confmat = confusion_matrix(y_true=y_test, y_pred=y_pred, labels=[0,1])\n",
        "fig, ax = plt.subplots(figsize=(3,3))\n",
        "ax.matshow(confmat, cmap=plt.cm.Blues, alpha=0.3)\n",
        "for i in range(confmat.shape[0]):\n",
        "    for j in range(confmat.shape[1]):\n",
        "        ax.text(x=j, y=i, s=confmat[i, j], va='center', ha='center')\n",
        "plt.xlabel('predicted label')\n",
        "plt.ylabel('true label')\n",
        "plt.tight_layout()\n",
        "plt.savefig('confusion_matrix.png', dpi=300)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hN0ZhweC-tBj"
      },
      "source": [
        "# Acesso dados\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EOzj0W8euuN5"
      },
      "source": [
        "#preparação dos dados\n",
        "loaders = prepararDados(224)\n",
        "train_loader = loaders['train']\n",
        "batch = next(iter(train_loader))\n",
        "images, labels = batch\n",
        "\n",
        "print(labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MSXXvNmO2Xhj"
      },
      "source": [
        "# Acessando um batch de elementos do dataset\n",
        "batch = next(iter(train_loader))\n",
        "images, labels = batch\n",
        "print(images.shape)\n",
        "print(images[0])\n",
        "grid = torchvision.utils.make_grid(images, nrow=8)\n",
        "plt.figure(figsize=(20,20))\n",
        "plt.imshow(np.transpose(grid, (1,2,0)))\n",
        "print('Labels: ', labels)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Nd159lznt4L"
      },
      "source": [
        "# Acessando um batch de elementos do dataset\n",
        "batch = next(iter(train_loader))\n",
        "images, labels = batch\n",
        "\n",
        "for i in range(len(images)): \n",
        "  noise = randint(0,2)\n",
        "  if noise == 1:\n",
        "    #aplica ruído gaussiano à imagem\n",
        "    images[i] = torch.tensor(random_noise(images[i], mode='gaussian', mean=0, var=0.05, clip=True))\n",
        "  if noise == 2:\n",
        "    #aplica speckle noise à imagem\n",
        "    images[i] = torch.tensor(random_noise(images[i], mode='speckle', mean=0, var=0.05, clip=True))\n",
        "\n",
        "\n",
        "print(images.shape)\n",
        "grid = torchvision.utils.make_grid(images, nrow=8)\n",
        "plt.figure(figsize=(20,20))\n",
        "plt.imshow(np.transpose(grid, (1,2,0)))\n",
        "print('Labels: ', labels)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fez4DhCUAIt7"
      },
      "source": [
        "# Modelo Estudante\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0XPPlJuyAKv1"
      },
      "source": [
        "class Network(nn.Module):\n",
        "    def __init__(self):\n",
        "        \"\"\"CNN Builder.\"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv_layer = nn.Sequential(\n",
        "            # Conv Layer block 1\n",
        "            nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "            # Conv Layer block 2\n",
        "            nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(in_channels=32, out_channels=24, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            \n",
        "           \n",
        "        )\n",
        "\n",
        "        self.fc_layer = nn.Sequential(\n",
        "           \n",
        "            nn.Linear(14*14*24, 2),\n",
        "       \n",
        "         \n",
        "        )\n",
        "\n",
        "    def forward(self, x):  \n",
        "\n",
        "        # conv layers\n",
        "        x = self.conv_layer(x)\n",
        "\n",
        "        # flatten\n",
        "        x = x.view(x.size(0), -1)\n",
        "\n",
        "        # fc layer\n",
        "        x = self.fc_layer(x)\n",
        "\n",
        "        return x\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UCT23jgx2FGO"
      },
      "source": [
        "model_name = 'aluno_50'\n",
        "\n",
        "num_epochs = 50\n",
        "\n",
        "aluno = Network()\n",
        "\n",
        "loaders = prepararDados(224)\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "aluno = aluno.to(device)\n",
        "\n",
        "# Observe that all parameters are being optimized\n",
        "optimizer = optim.Adam(aluno.parameters(), lr=0.001, weight_decay=0.000125)\n",
        "\n",
        "# Setup the loss fxn\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "print(aluno)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2wrTI2Fr2TXN"
      },
      "source": [
        "# Train and evaluate\n",
        "aluno, hist = train_model(aluno, loaders, criterion, optimizer, model_name, num_epochs=num_epochs, is_inception=(model_name==\"inception\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pZoIMhlrxEKF"
      },
      "source": [
        "aluno = carregar_cnn('/content/drive/My Drive/Modelos Salvos/aluno_50.acc')\n",
        "print('Modelo estudante - Test set')\n",
        "ini = time.time()\n",
        "metricasModelo(aluno, loaders['test'])\n",
        "print(time.time() - ini)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ph5it9PLdaot"
      },
      "source": [
        "model_name = 'aluno_kd_v50'\n",
        "\n",
        "ensemble = carregar_ensemble()\n",
        "\n",
        "num_epochs = 50\n",
        "\n",
        "aluno = Network()\n",
        "\n",
        "loaders = prepararDados(224)\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "aluno = aluno.to(device)\n",
        "\n",
        "# Observe that all parameters are being optimized\n",
        "optimizer = optim.Adam(aluno.parameters(), lr=0.001, weight_decay=0.000125)\n",
        "\n",
        "T = 2\n",
        "alpha = 0.9\n",
        "\n",
        "# T=2 alpha = 0.9\n",
        "aluno = train_model_kd_ensemble(aluno, ensemble, loaders, optimizer, model_name, T, alpha, num_epochs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9vHa6FYGxSvs"
      },
      "source": [
        "print('Modelo estudante - Train set')\n",
        "metricasModelo(aluno, loaders['train'])\n",
        "print('Modelo estudante - Test set')\n",
        "metricasModelo(aluno, loaders['test'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3XS5lfi0XdGC"
      },
      "source": [
        "# Treinamento KD\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nPPkaCbwXbqG"
      },
      "source": [
        "def train_model_kd(model, teacher, dataloaders, optimizer, save_name, T, alpha, num_epochs=25):\n",
        "  since = time.time()\n",
        "  \n",
        "  # informações para plotagem\n",
        "  train_losses , train_accuracy = [],[]\n",
        "  val_losses , val_accuracy = [],[]\n",
        "\n",
        "  # Gerencia a Learning Rate\n",
        "  scheduler = StepLR(optimizer, step_size=3, gamma=0.5)\n",
        "\n",
        "  best_model_wts = copy.deepcopy(model.state_dict())\n",
        "  best_acc = 0.0  \n",
        "\n",
        "  for epoch in range(1, num_epochs+1):\n",
        "    print('Epoch {}/{}'.format(epoch, num_epochs))\n",
        "    print('-' * 10)\n",
        "\n",
        "    epochStart = time.time() \n",
        "    # Each epoch has a training and validation phase\n",
        "    for phase in ['train', 'val']:\n",
        "            \n",
        "      if phase == 'train':\n",
        "          model.train()  # Set model to training mode\n",
        "      else:\n",
        "          model.eval()   # Set model to evaluate mode\n",
        "\n",
        "      running_loss = 0.0\n",
        "      running_corrects = 0\n",
        "      epoch_loss = 0.0\n",
        "      epoch_acc = 0.0\n",
        "\n",
        "      # Iterate over data.\n",
        "      for inputs, labels in dataloaders[phase]:    \n",
        "        \n",
        "        inputs = inputs.cuda()\n",
        "        labels = labels.cuda()\n",
        "        \n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward\n",
        "        # track history if only in train\n",
        "        with torch.set_grad_enabled(phase == 'train'):          \n",
        "          outputs = model(inputs)          \n",
        "          \n",
        "          if phase == 'train':\n",
        "            teacher_outputs = teacher(inputs)\n",
        "            loss = loss_fn_kd(outputs, labels, teacher_outputs, T, alpha)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "          else: \n",
        "            loss = F.cross_entropy(outputs, labels)      \n",
        "          \n",
        "          _, preds = torch.max(outputs, 1)\n",
        "         \n",
        "          # statistics\n",
        "          running_loss += loss.item() * inputs.size(0)\n",
        "          running_corrects += torch.sum(preds == labels.data)\n",
        "        \n",
        "      epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
        "      epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)      \n",
        "      \n",
        "      if phase == 'train':\n",
        "        train_losses.append(epoch_loss)\n",
        "        train_accuracy.append(epoch_acc.item())         \n",
        "      else:\n",
        "        val_losses.append(epoch_loss)\n",
        "        val_accuracy.append(epoch_acc.item())    \n",
        "\n",
        "      print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))      \n",
        "\n",
        "      # deep copy the model\n",
        "      if phase == 'val' and epoch_acc >= best_acc:\n",
        "        best_acc = epoch_acc\n",
        "        best_model_wts = copy.deepcopy(model.state_dict())\n",
        "        salvar_modelo(model, '/content/drive/My Drive/Modelos Salvos/'+save_name+'.acc')\n",
        "        print('Val acc up --> saving model')\n",
        "        \n",
        "    scheduler.step()\n",
        "    epochEnd = time.time()\n",
        "    print('Duração:', (epochEnd - epochStart))\n",
        "  time_elapsed = time.time() - since\n",
        "  print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
        "  print('Best val Acc: {:4f}'.format(best_acc))\n",
        "  \n",
        "  #plota os gráficos\n",
        "  plotar(train_losses, train_accuracy, val_losses, val_accuracy)\n",
        "\n",
        "  #salva dados da plotagem\n",
        "  train = {'loss': train_losses, 'acc': train_accuracy}\n",
        "  valid = {'loss': val_losses, 'acc': val_accuracy}\n",
        "\n",
        "  df = pd.DataFrame(data=train)\n",
        "  df.to_csv('/content/drive/My Drive/Modelos Salvos/'+save_name+'_train.csv')\n",
        "\n",
        "  df = pd.DataFrame(data=valid)\n",
        "  df.to_csv('/content/drive/My Drive/Modelos Salvos/'+save_name+'_valid.csv')\n",
        "  \n",
        "  # load best model weights\n",
        "  model.load_state_dict(best_model_wts)\n",
        "  return model\n",
        "\n",
        "def loss_fn_kd(outputs, labels, teacher_outputs, T, alpha): \n",
        "\n",
        "  return nn.KLDivLoss()(F.log_softmax(outputs/T, dim=1), F.softmax(teacher_outputs/T, dim=1)) * (alpha * T * T) + F.cross_entropy(outputs, labels) * (1. - alpha)\n",
        "\n",
        "def train_model_kd_ensemble(model, ensemble, dataloaders, optimizer, save_name, T, alpha, num_epochs=25):\n",
        "  since = time.time()\n",
        "\n",
        "  if (torch.cuda.is_available()):\n",
        "    resnet = ensemble['resnet']\n",
        "    resnet = resnet.cuda()\n",
        "    vgg = ensemble['vgg']\n",
        "    vgg = vgg.cuda()\n",
        "    densenet = ensemble['densenet']\n",
        "    densenet = densenet.cuda()\n",
        "    alexnet = ensemble['alexnet']\n",
        "    alexnet = alexnet.cuda()\n",
        "    googlenet = ensemble['googlenet']\n",
        "    googlenet = googlenet.cuda()\n",
        "  \n",
        "  resnet.eval()\n",
        "  densenet.eval()\n",
        "  vgg.eval()\n",
        "  alexnet.eval()\n",
        "  googlenet.eval()\n",
        "\n",
        "  \n",
        "  # informações para plotagem\n",
        "  train_losses , train_accuracy = [],[]\n",
        "  val_losses , val_accuracy = [],[]\n",
        "\n",
        "  # Gerencia a Learning Rate\n",
        "  scheduler = StepLR(optimizer, step_size=3, gamma=0.5)\n",
        "\n",
        "  best_model_wts = copy.deepcopy(model.state_dict())\n",
        "  best_acc = 0.0  \n",
        "\n",
        "  for epoch in range(1, num_epochs+1):\n",
        "    print('Epoch {}/{}'.format(epoch, num_epochs))\n",
        "    print('-' * 10)\n",
        "\n",
        "    epochStart = time.time() \n",
        "    # Each epoch has a training and validation phase\n",
        "    for phase in ['train', 'val']:\n",
        "            \n",
        "      if phase == 'train':\n",
        "          model.train()  # Set model to training mode\n",
        "      else:\n",
        "          model.eval()   # Set model to evaluate mode\n",
        "\n",
        "      running_loss = 0.0\n",
        "      running_corrects = 0\n",
        "      epoch_loss = 0.0\n",
        "      epoch_acc = 0.0\n",
        "\n",
        "      # Iterate over data.\n",
        "      for inputs, labels in dataloaders[phase]:    \n",
        "        \n",
        "        inputs = inputs.cuda()\n",
        "        labels = labels.cuda()\n",
        "        \n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward\n",
        "        # track history if only in train\n",
        "        with torch.set_grad_enabled(phase == 'train'):          \n",
        "          outputs = model(inputs)          \n",
        "          \n",
        "          if phase == 'train':\n",
        "            output_resnet = resnet(inputs)\n",
        "            output_densenet = densenet(inputs)\n",
        "            output_alexnet = alexnet(inputs)\n",
        "            output_googlenet = googlenet(inputs)\n",
        "            output_vgg = vgg(inputs)\n",
        "\n",
        "            teacher_outputs = (output_resnet+output_densenet+output_alexnet+output_googlenet+output_vgg)/5\n",
        "            \n",
        "            loss = loss_fn_kd(outputs, labels, teacher_outputs, T, alpha)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "          else: \n",
        "            loss = F.cross_entropy(outputs, labels)      \n",
        "          \n",
        "          _, preds = torch.max(outputs, 1)\n",
        "         \n",
        "          # statistics\n",
        "          running_loss += loss.item() * inputs.size(0)\n",
        "          running_corrects += torch.sum(preds == labels.data)\n",
        "        \n",
        "      epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
        "      epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)      \n",
        "      \n",
        "      if phase == 'train':\n",
        "        train_losses.append(epoch_loss)\n",
        "        train_accuracy.append(epoch_acc.item())         \n",
        "      else:\n",
        "        val_losses.append(epoch_loss)\n",
        "        val_accuracy.append(epoch_acc.item())    \n",
        "\n",
        "      print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))      \n",
        "\n",
        "      # deep copy the model\n",
        "      if phase == 'val' and epoch_acc >= best_acc:\n",
        "        best_acc = epoch_acc\n",
        "        best_model_wts = copy.deepcopy(model.state_dict())\n",
        "        salvar_modelo(model, '/content/drive/My Drive/Modelos Salvos/'+save_name+'.acc')\n",
        "        print('Val acc up --> saving model')\n",
        "        \n",
        "    scheduler.step()\n",
        "    epochEnd = time.time()\n",
        "    print('Duração:', (epochEnd - epochStart))\n",
        "  time_elapsed = time.time() - since\n",
        "  print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
        "  print('Best val Acc: {:4f}'.format(best_acc))\n",
        "  \n",
        "  #plota os gráficos\n",
        "  plotar(train_losses, train_accuracy, val_losses, val_accuracy)\n",
        "\n",
        "  #salva dados da plotagem\n",
        "  train = {'loss': train_losses, 'acc': train_accuracy}\n",
        "  valid = {'loss': val_losses, 'acc': val_accuracy}\n",
        "\n",
        "  df = pd.DataFrame(data=train)\n",
        "  df.to_csv('/content/drive/My Drive/Modelos Salvos/'+save_name+'_train.csv')\n",
        "\n",
        "  df = pd.DataFrame(data=valid)\n",
        "  df.to_csv('/content/drive/My Drive/Modelos Salvos/'+save_name+'_valid.csv')\n",
        "  \n",
        "  # load best model weights\n",
        "  model.load_state_dict(best_model_wts)\n",
        "  return model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fLaecNCiZyaZ"
      },
      "source": [
        "# Parâmetros\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mMURvAlhucxw"
      },
      "source": [
        "def infoModelo(model):\n",
        "  pytorch_total_params = 0\n",
        "  for p in model.parameters(): \n",
        "    pytorch_total_params += p.numel()\n",
        "\n",
        "  pytorch_trainable_params = 0\n",
        "  for p in model.parameters(): \n",
        "    if p.requires_grad:\n",
        "      pytorch_trainable_params += p.numel()\n",
        " \n",
        "  print('Parâmetros treináveis: ', pytorch_trainable_params)\n",
        "  print('Parâmetros não treináveis: ', pytorch_total_params-pytorch_trainable_params)\n",
        "  print('Total: ', pytorch_total_params)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rSclbTakZ1Os"
      },
      "source": [
        "vgg = models.vgg19_bn(pretrained=True)\n",
        "for param in vgg.features.parameters():\n",
        "  param.requires_grad = False\n",
        "num_ftrs = vgg.classifier[6].in_features\n",
        "vgg.classifier[6] = nn.Linear(num_ftrs, 2)\n",
        "infoModelo(vgg)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GEbxyzbYp6br"
      },
      "source": [
        "resnet, input_size = initialize_model('resnet', 2, True, use_pretrained=True)\n",
        "for param in resnet.layer4.parameters():\n",
        "  param.requires_grad = True\n",
        "infoModelo(resnet)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G7ruJ2uVuccj"
      },
      "source": [
        "densenet, input_size = initialize_model('densenet', 2, True, use_pretrained=True)\n",
        "for param in densenet.features.denseblock4.denselayer15.parameters():\n",
        "  param.requires_grad = True\n",
        "\n",
        "for param in densenet.features.denseblock4.denselayer16.parameters():\n",
        "  param.requires_grad = True\n",
        "infoModelo(densenet)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YTnKQxTUxG36"
      },
      "source": [
        "model_ft, input_size = initialize_model('inception', 2, True, use_pretrained=True)\n",
        "for param in model_ft.Mixed_7c.parameters():\n",
        "  param.requires_grad = True\n",
        "infoModelo(model_ft)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hCGsXPy9yVOT"
      },
      "source": [
        "model_ft, input_size = initialize_model('alexnet', 2,True , use_pretrained=True)\n",
        "infoModelo(model_ft)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h_1bFxuLyhZx"
      },
      "source": [
        "model_ft = models.googlenet(pretrained=True)\n",
        "set_parameter_requires_grad(model_ft, True)\n",
        "for param in model_ft.inception5b.parameters():\n",
        "  param.requires_grad = True\n",
        "num_ftrs = model_ft.fc.in_features\n",
        "model_ft.fc = nn.Linear(num_ftrs, 2)\n",
        "infoModelo(model_ft)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2AMOnpLbudA-"
      },
      "source": [
        "cnn = Network()\n",
        "infoModelo(cnn)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}